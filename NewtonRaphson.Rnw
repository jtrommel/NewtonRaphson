\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Newton-Raphson method}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=NR}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(gridExtra)
library(scatterplot3d)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions

\frontmatter
\chapter*{Newton-Raphson method for finding the maximum of a function}

\mainmatter
\chapter{Function of one variable}

\section{Taylor approximation}
Taylor's Theorem says that if a function $f(x)$ is $(k+1)$ times differentiable on an open interval I, than for any two points $x$ and $x+h$ in I there exists a point $w$ between $x$ and $x+h$ such that:
\begin{equation}
f(x+h)=f(x) + f'(x)h + \frac{1}{2}f''(x)h^{2}+ \ldots + \frac{1}{k!}f^{k}(x)h^{k} + \frac{1}{(k+1)!}f^{k+1}(w)h^{k+1}
\label{Taylor1v}
\end{equation}

For small values of $h$ (in the limit when $h$ goes to $0$) the higher order terms in \ref{eq:Taylor1v} go to $0$ much faster than $h$ itself. In that case (small values of $h$) we can approximate the function, keeping only a restricted number of terms. The first and second order Taylor approximations are
\begin{equation}
  \begin{split}
  f(x) &\approx f(x+h) + f'(x)h \\
  f(x) &\approx f(x=h) + f'(x)h + \frac{1}{2}f''(x)h^{2}
  \end{split}
\end{equation}

\section{Finding the maximum of the function}
A function $f(x)$ reaches a (local) maximum for $x=\hat{x}$ when
\begin{equation}
  \begin{split}
  f'(\hat{x})&=0 \\
  f''(\hat{x})&<0
  \end{split}
\end{equation}

Suppose that $\hat{x}$, where $f(x)$ reaches a maximum, lies close to point $x=x_{0}$. Then we can use the second order Taylor approximation in the neighbourhood of $x_{0}$:
\begin{equation}
f(x_{0}+h) \approx f(x_{0}) + f'(x_0)h + \frac{1}{2}f''(x_{0})h^{2}
\end{equation}

The value $\hat{h}$ for which $\hat{x}=x_{0}+\hat{h}$ can then be found from
\begin{equation}
\frac{d}{dh}f(x_{0}+\hat(h)=f'(x_{0})+f''(x_{0})\hat{h}=0
\end{equation}

or
\begin{equation}
\hat{h}=-\frac{f'(x_{0})}{f''(x_{0})}
\label{eq:hat_h}
\end{equation}

This will not insure that $\hat{x}=x_{0}+\hat{h}$ will be where $f(x)$ reaches its maximum. This is because the second order Taylor approximation is just that: an approximation, and because it is only valid for small values of $h$. An iterative process is needed that gradually gets us to a value of $\hat{x}$ where the value of $f'(\hat{x})$ is close enough to zero. What ''close enough" is, is our own choice.

\section{Examples}

The examples are taken from the topic of the Likelihood Function in statistics. There are different methods for estimating model parameters of a distribution. One of these methods uses the Likelihood Function (or its logaritm). Sometimes an analytical solution is possible, but sometimes we have to use a numerical technique such as the Newton-Raphson method.

\subsection{Example 1: Estimating the mean $\mu$ of a normal distribution with known variance $\sigma^{2}$}

This is a classic starting point for estimating parameters in statistics textbooks.

\newthought{We can find an unbiased estimator as follows}:
\newline
We have $X \sim \mathcal{N}(\mu,\,\sigma^{2})$. We take a sample $(x_{1}, x_{2}, \ldots x_{n})$ of size $n$ from this distribution. Then we know that $X_{1} \sim \mathcal{N}(\mu,\,\sigma^{2})$, $X_{2} \sim \mathcal{N}(\mu,\,\sigma^{2})$, $\ldots$ $X_{n} \sim \mathcal{N}(\mu,\,\sigma^{2})$. This means that the derived chance variable $\bar{X}=\frac{1}{n}(X_{1}+X_{2} + \ldots + X_{n})$ will be $\bar{X} \sim \mathcal{N}(\mu,\,\frac{\sigma^{2}}{n})$. Therefore $\bar{x}= \frac{1}{n}(x_{1}+x_{2} + \ldots + x_{n})$ will be an unbiased estimater of $\mu$.
\begin{equation}
estimate(\mu)=\bar{x}=\frac{\sum_{i=1}^{i=n}x_{i}}{n}
\label{eq:estimate_xbar}
\end{equation}

\newthought{The Method of the Maximum Likelihood} takes a different approach. Suppose we estimate that the value for $\mu$ is $\mu_1$. Knowing that $X \sim \mathcal{N}(\mu,\,\sigma^{2})$, we can calculate the probability of our first observation in the sample:
\begin{equation}
P[X=x_{1}|\mu=\mu_{1}]=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x_{1}-\mu_{1}}{\sigma})^{2}}
\end{equation}

For all the other observations in the sample we can calculate their probability. Assuming that all observations are independent, the total probability of having this specific sample with this particular estimate of $\mu=\mu_{1}$, will be equal to the product of all the individual probabilities. We call this the Likelihood of this sample:
\begin{equation}
\mathcal{L}(x_{1},x_{2}, \ldots x_{n})|\mu=\mu_{1})=\prod_{i=1}^{i=n}P[X=x_{i}|\mu=\mu_{1}]=\prod_{i=1}^{i=n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x_{i}-\mu_{1}}{\sigma})^{2}}
\end{equation}

For a different estimate $\mu=\mu_{2}$ we will get a different value for the Likelihood of the sample. In general: the Likelihood of the sample is a function of $\mu$:
\begin{equation}
\mathcal{L}(x_{1},x_{2}, \ldots x_{n})|\mu)=\prod_{i=1}^{i=n}P[X=x_{i}|\mu]=\prod_{i=1}^{i=n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x_{i}-\mu}{\sigma})^{2}}
\end{equation}

Instead of working with the \emph{Likelihood function}, it is easier to work with the (natural) logarithm of this function, the \emph{Log Likelihood function}:
\begin{equation}
\ell((x_{1},x_{2}, \ldots x_{n})|\mu)=ln(\mathcal{L}(x_{1},x_{2}, \ldots x_{n})|\mu))=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) + \sum_{i=1}^{i=n} -\frac{1}{2}\left( \frac{x_{i}-\mu}{\sigma} \right) ^{2}
\end{equation}

Both functions have a maximum for the same value of $\mu=\hat{\mu}$. We find $\hat{\mu}$ from
\begin{equation}
\frac{d}{d\mu}\ell((x_{1},x_{2}, \ldots x_{n})|\hat{\mu})= \sum_{i=1}^{i=n}\frac{x_{i}-\hat{\mu}}{\sigma}=0
\end{equation}
or
\begin{equation}
\hat{\mu}=\frac{\sum_{i=1}^{i=n}x_{i}}{n}=\bar{x}
\end{equation}

In this case the Method of Maximum Likelihood leads to an equation that can be solved analytically and it gives the same result as in equation \ref{eq:estimate_xbar}.

\newthought{Finding the maximum of the Log Likelihood function} is not always possible in an analytical way. Then we have to use a numerical method such as Newton-Raphson. For a numerical solution we need to have a specific situation. Suppose we take a sample of size 10 from a normal distribution with $\mu=100$ and $\sigma=10$:
<<>>=
set.seed(2018)
n <- 10
mu <- 100
sigma <- 10
x <- rnorm(n,mu,sigma)
@
Then we calculate the Log Likelihood function for 100 values of $\mu$ ranging from $\mu-2\sigma$ to $\mu+2\sigma$ and make a plot of the Log Likelihood function:
<<label=llnorm,fig=TRUE,include=FALSE, echo=TRUE>>=
# ll = Log Likelihood function of mu
ll <- data.frame(mu=seq(mu-2*sigma,mu+2*sigma,length.out = 100),log.likelihood=0)
for (i in 1:nrow(ll)) {
  ll$log.likelihood[i] <- - (1/(2*sigma^2))*sum((x-ll$mu[i])^2)
}
ll$log.likelihood <- ll$log.likelihood - n*log(sqrt(2*pi)*sigma)
ggplot(data=ll, aes(x=mu,y=log.likelihood)) +
  geom_line() +
  labs(title = "Log Likelihood of sample (size=10) from N(100,10)",
       x = expression(mu),
       y = "ll(mu)") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{NR-llnorm}
\caption{}
\label{fig:llnorm}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

The iteration starts from a value $\mu_{0}$. From there we calculate $\hat{h}$ so that the maximum will be found at $\hat{\mu}=\mu_{0}+\hat{h}$. Intu\"{i}tively, most people would choose $\mu_{0}=\bar{x}$ (and they would be spot on from the first try!). Let us be naive and pick $\mu_{0}=min(x_{i})$. Equation \ref{eq:hat_h} gives us the value of $\hat{h}$ based on the first and second derivative of $\ell(\mu)$. The Log Likelihood function, its 1st and 2nd derivative in this case are:
\begin{equation}
\begin{split}
\ell((x_{1},x_{2}, \ldots x_{n})|\mu)&=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) - \sum_{i=1}^{i=n} \frac{1}{2}\left( \frac{x_{i}-\mu}{\sigma} \right) ^{2} \\
\frac{d}{d\mu}\ell((x_{1},x_{2}, \ldots x_{n})|\mu)&=\sum_{i=1}^{i=n}\left( \frac{x_{i}-\mu}{\sigma^{2}} \right)\\
\frac{d^{2}}{d\mu^{2}}\ell((x_{1},x_{2}, \ldots x_{n})|\mu)&= - \frac{n}{\sigma^{2}}
\end{split}
\end{equation}

We can see that the second derivative is constant and negative. This means that the extremum will be a maximum (as we can see from Figure~\ref{fig:llnorm}). We will check is the first derivative is zero for $\hat{\mu}$. It probably will not be exactly zero so we have to set a level under which the iteration will be stopped.
<<>>=
#
# Numerical solution with 2nd order Tayler approximation of the log likelihood function
#
level <- 1e-5
# Starting value
mu0 <- min(x)
#
beneath_level <- FALSE
# Second derivative is a constant
df2 <- -n/(sigma^2)
# We keep a record of the values that mu takes on during the iteration
mu.reposit <- data.frame(mu=mu0)
while(!beneath_level) {
  df1 <- sum(x-mu0)/(sigma^2)
  # Calculate mu_hat
  mu_hat <- mu0 - df1/df2
  # Add new value to the repository of mu
  mu.reposit <- rbind(mu.reposit,mu_hat)
  # Calculate the first derivative and check if it is close enough to zero (beneath the level)
  df1 <- sum(x-mu_hat)/(sigma^2)
  if (abs(df1) < level) {
    beneath_level <- TRUE
  }
  # Put the new value of mu into mu0
  mu0 <- mu_hat
}
paste("The numerical solution is: mu=",mu0)
paste("The analytical solution is: xbar=",mean(x))
mu.reposit
@

As we can see, the numerical iteration needed only one step to find the exact solution. This is because the Log Likelihood function in this case is given by:
\begin{equation}
\ell((x_{1},x_{2}, \ldots x_{n})|\mu)=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) - \sum_{i=1}^{i=n} \frac{1}{2}\left( \frac{x_{i}-\mu}{\sigma} \right) ^{2}
\end{equation}
This is a quadratic equation in $\mu$, which means that its second order Taylor approximation won't be an approximation, but will be exactly equal to the function.

\subsection{Example 2: Estimating the probability $\pi$ of a binomial experiment}

When we do an binomial experiment we do $n$ tries where each try can only have two outcomes: ''success" of "failure". Let $X$ be the chance variable that is equal to the number of successes, than this number will depend on the probability of success $\pi$ in each try. $X$ has a Binomial distribution $X \sim \mathcal{B}(\pi)$ and the probability that $X=k$ is given by:
\begin{equation}
P[X=k|\pi]={n \choose k}\pi^{k}(1-\pi)^{(n-k)}
\end{equation}

Suppose we do an experiment with $n=10$ tries. The results of the 10 tries are: $(0, 1, 1, 1, 0, 1, 1, 1, 1, 1)$. The number of successes is $X=8$. Can we make an estimate of $\pi$ based on this result?

The Likelihood for obtaining a ''0" in the first experiment is:
\begin{equation}
P[x_{1}=0|\pi]=(1-\pi)
\end{equation}

The Likelihood for obtaining all the registred results is:
\begin{equation}
\begin{split}
P[x_{1}&=0|\pi]=(1-\pi) \\
P[x_{2}&=1|\pi]=\pi \\
P[x_{3}&=1|\pi]=\pi \\
P[x_{4}&=1|\pi]=\pi \\
P[x_{5}&=0|\pi]=(1-\pi) \\
P[x_{6}&=1|\pi]=\pi \\
P[x_{7}&=1|\pi]=\pi \\
P[x_{8}&=1|\pi]=\pi \\
P[x_{9}&=1|\pi]=\pi \\
P[x_{10}&=1|\pi]=\pi
\end{split}
\end{equation}

The Likelihood for this result is the product of the probabilities per try:
\begin{equation}
\mathcal{L}(x_{1},x_{2}, \ldots x_{10})|\pi)=\mathcal{L}(X=k|\pi)=\prod_{i=1}^{i=10}P[x_{i}|\pi]=\pi^{8}(1-\pi)^{2}=\pi^{k}(1-\pi)^{n-k}
\end{equation}

The Log Likelihood function is:
\begin{equation}
\ell(k|\pi)=k.ln(\pi) + (n-k).ln((1-\pi))
\end{equation}

It looks like this (Figure~\ref{fig:llbinom})
<<label=llbinom,fig=TRUE,include=FALSE, echo=TRUE>>=
n <- 10
x <- c(0, 1, 1, 1, 0, 1, 1, 1, 1, 1)
k <- sum(x)
# ll = Log Likelihood function of pi
ll <- data.frame(p=seq(0.01, 0.99, length.out = 100),log.likelihood=0)
ll$log.likelihood <- k*log(ll$p) + (n - k)*log(1-ll$p)
ggplot(data=ll, aes(x=p,y=log.likelihood)) +
  geom_line() +
  labs(title = "Log Likelihood of binomial experiment",
       x = expression(pi),
       y = "ll(pi)") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{NR-llbinom}
\caption{}
\label{fig:llbinom}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

The Log Likelihood function, its first and second derivatives are:
\begin{equation}
\begin{split}
\ell(k|\pi) &=k.ln(\pi) + (n-k).ln(1-\pi) \\
\frac{d}{d\pi}\ell(k)|\pi) &=\frac{k}{\pi} - \frac{n-k}{1-\pi} \\
\frac{d^{2}}{d\pi^{2}}\ell(k|\pi) &=-\frac{k}{\pi^{2}} -  \frac{n-k}{(1-\pi)^{2}}
\end{split}
\end{equation}

Again, the search for the $\pi$-value for which the Log Likelihood function reaches a maximum (we can see that the second derivative is always negative), has an analytical solution because $\frac{d}{d\pi}\ell(k|\pi)=\frac{k}{\pi} -  \frac{n-k}{1-\pi}=0$ for $\pi=\frac{k}{n}=0.8$.

We can find this also numerically, but it will not be so easy because the Log Likelihood function is clearly not quadratic.

<<>>=
#
# Numerical solution with 2nd order Tayler approximation of the log likelihood function
#
level <- 1e-5
# Starting value
p0 <- min(ll$p)
#
beneath_level <- FALSE
# We keep a record of the values that p takes on during the iteration
p.reposit <- data.frame(p=p0)
while(!beneath_level) {
  df1 <- k/p0 - (n - k)/(1 - p0)
  df2 <- -k/(p0^2) - (n - k)/((1 - p0)^2)
  # Calculate p_hat
  p_hat <- p0 - df1/df2
  # Add new value to the repository of p
  p.reposit <- rbind(p.reposit,p_hat)
  # Calculate the first derivative and check if it is close enough to zero (beneath the level)
  df1 <- k/p_hat - (n - k)/(1 - p_hat)
  if (abs(df1) < level) {
    beneath_level <- TRUE
  }
  # Put the new value of p into p0
  p0 <- p_hat
}
paste("The numerical solution is: pi=",p0)
paste("The analytical solution is: pi=",k/n)
p.reposit
@

We find the correct result, but the iteration takes much longer because the Taylor approximation in this case is truly an approximation!

\chapter{Function of two variables}
\section{Taylor approximation}
Taylor's Theorem says that if a function $f(x,y)$ is at least two times differentiable on an open interval I, than for any two points $(x,y)$ and $(x+h_{x}, y+h_{y}$ ($h_{x}$ and $h_{y}$ small) the second order Taylor approximation is given by:

\begin{equation}
f(x+h_{x},y+h_{y}) \approx f(x,y) + \frac{\partial}{\partial x}f(x,y).h_{x} + \frac{\partial}{\partial y}f(x,y).h_{y} +\frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}f(x,y).h_{x}^{2} + \frac{\partial^{2}}{\partial x \partial y}f(x,y).h_{x}.h_{y} +\frac{1}{2}\frac{\partial^{2}}{\partial y^{2}}f(x,y).h_{y}^{2}
\end{equation}

\section{Finding the maximum of a function of two variables}

A function $f(x,y)$ reaches a (local) maximum for $(x=\hat{x},y=\hat{y}$ when
\begin{equation}
\begin{cases}
  \frac{\partial}{\partial x}f(\hat{x},\hat{y})=0 \\
  \frac{\partial}{\partial y}f(\hat{x},\hat{y})=0
\end{cases}
\end{equation}

Suppose that $\hat{x}, \hat{y}$, where $f(x,y)$ reaches a maximum, lies close to point $x=x_{0},y=y_{0}$. Then we can use the second order Taylor approximation in the neighbourhood of $x_{0}$:
\begin{equation}
\begin{split}
f(x_{0}+h_{x},y_{0}+h_{y}) \approx f(x_{0},y_{0}) & + \frac{\partial}{\partial x}f(x_{0},y_{0}).h_{x} + \frac{\partial}{\partial y}f(x_{0},y_{0}).h_{y}\\
& +\frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}f(x_{0},y_{0}).h_{x}^{2} + \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{x}.h_{y} +\frac{1}{2}\frac{\partial^{2}}{\partial y^{2}}f(x_{0},y_{0}).h_{y}^{2}
\end{split}
\end{equation}

The values $\hat{h_{x}}$ and  $\hat{h_{y}}$ for which $\hat{x}=x_{0}+\hat{h_{x}}$ and $\hat{y}=y_{0}+\hat{h_{y}}$ can then be found from the following set of equations:
\begin{equation}
  \begin{cases}
    \frac{\partial}{\partial h_{x}}f(x_{0}+\hat{x},y_{0}+\hat{y})= \frac{\partial}{\partial x}f(x_{0},y_{0}) + \frac{\partial^{2}}{\partial x^{2}}f(x_{0},y_{0}).h_{x} + \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{y}=0 \\
    \frac{\partial}{\partial h_{y}}f(x_{0}+\hat{x},y_{0}+\hat{y})= \frac{\partial}{\partial y}f(x_{0},y_{0}) + \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{x} + \frac{\partial^{2}}{\partial y^{2}}f(x_{0},y_{0}).h_{y}=0
  \end{cases}
\end{equation}

or
\begin{equation}
  \begin{cases}
    \frac{\partial^{2}}{\partial x^{2}}f(x_{0},y_{0}).h_{x} + \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{y}= - \frac{\partial}{\partial x}f(x_{0},y_{0}) \\
    \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{x} + \frac{\partial^{2}}{\partial y^{2}}f(x_{0},y_{0}).h_{y}= -\frac{\partial}{\partial y}f(x_{0},y_{0})
  \end{cases}
\label{eq:set}
\end{equation}

We can solve this set of equations quickly in R using matrices. We can write the set of equations \ref{eq:set} as follows:
\begin{equation}
\textbf{A} \boldsymbol{\cdot} \textbf{h} =\textbf{b}
\end{equation}

with
\begin{equation}
  \textbf{A}= \begin{bmatrix} \frac{\partial^{2}}{\partial x^{2}}f(x_{0},y_{0}) & \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}) \\ \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}) & \frac{\partial^{2}}{\partial y^{2}}f(x_{0},y_{0}) \end{bmatrix} = \textbf{Hess}(f(x_{0},y_{0}))
\end{equation}
the \emph{Hessian} matrix of $f(x,y)$ in $(x_{0},y_{0})$

and
\begin{equation}
  \textbf{b}= \begin{bmatrix} - \frac{\partial}{\partial x}f(x_{0},y_{0}) \\ - \frac{\partial}{\partial y}f(x_{0},y_{0}) \end{bmatrix} = \boldsymbol{\nabla}(f(x_{0},y_{0}))
\end{equation}
the \emph{gradient} vector of $f(x,y)$ in $(x_{0},y_{0})$

and
\begin{equation}
  \textbf{h}= \begin{bmatrix} h_{x} \\ h_{y} \end{bmatrix}
\end{equation}
the vector of the values we are calculating.

Again, this will not insure that $(\hat{x}=x_{0}+\hat{h_{x}},\hat{y}=y_{0}+\hat{h_{y}}$ will be where $f(x,y)$ reaches its maximum. The second order Taylor approximation remains an approximation, and is only valid for small values of $(h_{x},h_{y})$. Using an iterative process we can find out if $\frac{\partial}{\partial x}f(\hat{x},\hat{y}) and \frac{\partial}{\partial y}f(\hat{x},\hat{y})$ are close enough to zero. We will use the RMS value to test this condition with
\begin{equation}
RMS = \sqrt{\frac{\partial}{\partial x}f(\hat{x},\hat{y})^{2}+\frac{\partial}{\partial y}f(\hat{x},\hat{y})^{2}}
\end{equation}

\section{Examples}

\subsection{Example 1: Estimating the co\"effici\"ents of a linear regressio model}

Choosing a linear regression model means that we assume that the real relation between a response variable $Y$ and an independent variable $X$ has the form:
\begin{equation}
Y = \beta_{0}+\beta_{1}X
\end{equation}

However, when we take $n$ measurements of couples $(x_{i},y_{i})$ these, in general, do not lie on a straigth line. This is considered to be caused by other unknown or uncontrollable variables. However we assume that the net effect of these disturbances is normally distributed $U \sim \mathcal{N}(0,\sigma^{2})$, and that the value of $\sigma$ is independent of $X$\sidenote{LineaireRegressie.pdf}.
\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}X_{i}+U_{i}
\end{equation}
Because $\beta_{0}$, $\beta_{1}$ and $X_{i}$ are constants or known values this implies that $Y_{i}$ is normally distributed: $Y_{i} \sim \mathcal{N}(\beta_{0}+\beta_{1}X_{i},\sigma^{2})$

\newthought{The classical method} to estimate $\beta_{0}$ and $\beta_{1}$ is the \emph{Ordinary Least Squares Method}. It minimises $Q$, the sum of the square of the differences between the observed value of $Y$ and the calculated value $\hat{Y}$. 
\begin{equation}
Q=\sum\limits_{i=1}^{i=n}\left( Y_{i}-\beta_{0}-\beta_{1}x_{i}  \right)^{2}
\end{equation}

The minimum is calculated by taking the partial derivatives of $Q$ with respect to $\beta_0$ and $\beta_{1}$, setting these equal to 0, and solving this set of two equations. These two equations are called the \emph{normal equations} \sidenote{Because the calculation is based on one set of $n$ observations, we will not find $\beta_{0}$ and $\beta_{1}$, but estimates $b_{0}$ and $b_{1}$ based on this sample}:
\begin{equation}
\begin{cases}
nb_{0}+b_{1}\sum_{i=1}^{i=n}x_{i}&=\sum_{i=1}^{i=n}y_{i} \\
b_{0}\sum_{i=1}^{i=n}x_{i}+b_{1}\sum_{i=1}^{i=n}x_{i}^{2}&=\sum_{i=1}^{i=n}x_{i}y_{i}
\end{cases}
\label{eq:normaleq}
\end{equation}

The solutions of the \emph{normal equations} are:
\begin{equation}
\begin{split}
b_{0}&=\bar{y}-b_{1}\bar{x}\\
b_{1}&=\frac{\sum\limits_{i=1}^{i=n} (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum\limits_{i=1}^{i=n}(x_{i}-\bar{x})^{2}}
\end{split}
\end{equation}

The \emph{Gauss-Markov Theorem} proves that the estimates for $\beta_{0}$ and $\beta_{1}$ obtained by the \emph{Ordinary Least Squares Method} are unbiased and have the smallest variance compared with other estimating methods.

\newthought{The Maximum Likelihood Method} uses the assumption that $Y_{i} \sim \mathcal{N}(\beta_{0}+\beta_{1}X_{i},\sigma^{2})$ to calculate the probability of observing the value $y_{i}$ when we use $\beta_{0}=\beta_{0}^{*}$ and $\beta_{1}=\beta_{1}^{*}$ as estimates for $\beta_{0}$ and $\beta_{1}$:
\begin{equation}
P[Y=y_{i}|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}{*}]=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{y_{i}-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}{\sigma})^{2}}
\end{equation}

The \emph{Likelihood function} for the whole sample is:
\begin{equation}
\mathcal{L}((y_{1},y_{2}, \ldots y_{n})|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}^{*})=\prod_{i=1}^{i=n}P[Y=y_{i}|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}^{*}]=\prod_{i=1}^{i=n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{y_{1}-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}{\sigma})^{2}}
\end{equation}

and the \emph{Log Likelihood function} is:
\begin{equation}
\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}^{*})=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) + \sum_{i=1}^{i=n} -\frac{1}{2}\left( \frac{y_{1}-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}{\sigma} \right) ^{2}
\end{equation}

Another estimate for $\beta_{0}$ than $\beta_{0}^{*}$ and for $\beta_{1}$ than $\beta_{1}^{*}$ will give another value $\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})$. In short: the \emph{Log Likelihood function} is a function of $\beta_{0}$ and $\beta_{1}$. The maximum of the \emph{Log Likelihood function} is found from the system of equations that we find by setting the partial derivatives of $\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})$ with respect to $\beta_{0}$ and $\beta_{1}$ equal to zero:
\begin{equation}
\begin{cases}
\frac{\partial}{\partial \beta_{0}}\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})=\frac{1}{\sigma^{2}}\sum_{i=1}^{i=n}\left( y_{i}-\beta_{0}-\beta_{1}x_{i} \right)=0 \\
\frac{\partial}{\partial \beta_{1}}\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})=\frac{1}{\sigma^{2}}\sum_{i=1}^{i=n}\left( y_{i}-\beta_{0}-\beta_{1}x_{i} \right)x_{i}=0
\end{cases}
\end{equation}

This is the set of \emph{normal equations} (\ref{eq:normaleq}) that we found using the \emph{Ordinary Least Squares method}. The \emph{Maximum Likelihood method} has an analytical solution, and it is the same as the one found by the \emph{Ordinary Least Squares method}.

We can of course also use the numerical \emph{Newton-Raphson method} to find the maximum. The \emph{Log Likelihood function} is:
\begin{equation}
\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) + \sum_{i=1}^{i=n} -\frac{1}{2}\left( \frac{y_{1}-\beta_{0}-\beta_{1}x_{i}}{\sigma} \right) ^{2}
\end{equation}

The function is quadratic in $\beta_{0}$ and $\beta_{1}$, so the use of the second order Taylor approximation should give a solution after one iteration.

We start by generating a sample of $n$ observations of the form $y_{i}=\beta_{0}+\beta_{1}x_{i}+u_{i}$ where $U \sim \mathcal{N}(0,\sigma^{2})$ (Figure~\ref{fig:linear}).
\medskip
<<label=linear,fig=TRUE,include=FALSE, echo=TRUE>>=
set.seed(2018)
# Defining beta0, beta1 and sigma
beta0 <- 1
beta1 <- 2
sigma <- 1
# Creating the sample
n <- 10
x <- seq(1:n)
y <- beta0 + beta1*x + rnorm(n,0,sigma)
# Graph of (x,y)
df <- data.frame(x=x,y=y)
ggplot(data=df, aes(x=x,y=y)) +
  geom_point() +
  labs(title = "Observations",
       x = "x",
       y = "y") +
  JT.theme
@

\begin{marginfigure}[-3cm]
\includegraphics[width=1\textwidth]{NR-linear}
\caption{}
\label{fig:linear}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

The \emph{Log Likelihood function} as a function of $\beta_{O}$ and $\beta_{1}$ looks like this (Figure~\ref{fig:lllinear})
\medskip
<<label=lllinear,fig=TRUE,include=FALSE, echo=TRUE>>=
# Graph of log likelihood function as a function of betaO and beta1
k <- 25
b0 <- seq(beta0-5*abs(beta0),beta0+5*abs(beta0),length.out = k)
b1 <- seq(-2*beta1,2*beta1,length.out = k)
ll1 <- matrix(0, nrow=k, ncol=k)
for(i in (1:k)) {
  for (j in (1:k)) {
    ll1[i,j] <- sum(y - b0[i] - b1[j]*x)^2
  }
}
ll <- n*log(1/(sqrt(2*pi)*sigma)) - (1/(2*sigma^2))*ll1
persp(b0,b1,ll,theta=135,
      xlim=c(beta0-5.5*abs(beta0),beta0+5.5*abs(beta0)),
      ylim=c(-2.5*beta1,2.5*beta1),
      zlim=c(min(ll),10000),
      xlab="beta0",
      ylab="beta1",
      main="log likelihood function",
      col="lightblue",
      shade=0.25,
      ticktype="detailed")
@

\begin{marginfigure}[-3cm]
\includegraphics[width=1\textwidth]{NR-lllinear}
\caption{}
\label{fig:lllinear}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}
\medskip
To find the maximum of the \emph{Log Likelihood function} we need to calculate the \emph{gradient vector} and the \emph{Hessian matrix} of the function in a chosen point $(\beta_{0}^{*},\beta_{1}^{*})$:
\begin{equation}
\begin{split}
\boldsymbol{\nabla}(\ell(\beta_{0}^{*},\beta_{1}^{*})) &= \begin{bmatrix} - \frac{\partial}{\partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*})) \\ - \frac{\partial}{\partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*})) \end{bmatrix} \\
  \textbf{Hess}(\ell(\beta_{0}^{*},\beta_{1}^{*})) &= \begin{bmatrix} \frac{\partial^{2}}{\partial \beta_{0}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*}) & \frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*}) \\ \frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*}) & \frac{\partial^{2}}{\partial \beta_{1}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*}) \end{bmatrix}
\end{split}
\end{equation}

The elements of the \emph{gradient vector} are:
\begin{equation}
\begin{split}
\frac{\partial}{\partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=\frac{1}{\sigma^{2}}\sum_{i=1}^{i=n}(y_{i}-\beta_{0}^{*}-\beta_{1}^{*}x_{i}) \\
\frac{\partial}{\partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=\frac{1}{\sigma^{2}}\sum_{i=1}^{i=n}(y_{i}-\beta_{0}^{*}-\beta_{1}^{*}x_{i})x_{i})
\end{split}
\end{equation}

\newpage
\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}
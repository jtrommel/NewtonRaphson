\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Newton-Raphson method}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=NR}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(gridExtra)
library(scatterplot3d)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions

\frontmatter
\chapter*{Finding the maximum of the (log) likelihood function with the Newton-Raphson method}

\mainmatter
\chapter{Function of one variable}

\section{Taylor approximation}
Taylor's Theorem says that if a function $f(x)$ is $(k+1)$ times differentiable on an open interval I, than for any two points $x$ and $x+h$ in I there exists a point $w$ between $x$ and $x+h$ such that:
\begin{equation}
f(x+h)=f(x) + f'(x)h + \frac{1}{2}f''(x)h^{2}+ \ldots + \frac{1}{k!}f^{k}(x)h^{k} + \frac{1}{(k+1)!}f^{k+1}(w)h^{k+1}
\label{eq:Taylor1v}
\end{equation}

For small values of $h$ (in the limit for $h$ going to $0$) the higher order terms in \ref{eq:Taylor1v} go to $0$ much faster than $h$ itself. In that case we can approximate the function by keeping only a restricted number of terms. The first and second order Taylor approximations are
\begin{equation}
  \begin{split}
  f(x+h) &\approx f(x) + f'(x)h \\
  f(x+h) &\approx f(x) + f'(x)h + \frac{1}{2}f''(x)h^{2}
  \end{split}
\end{equation}

\section{Finding the maximum of the function}
A function $f(x)$ reaches a (local) maximum for $x=\hat{x}$ when
\begin{equation}
  \begin{split}
  f'(\hat{x})&=0 \\
  f''(\hat{x})&<0
  \end{split}
\end{equation}

Suppose that $\hat{x}$, where $f(x)$ reaches a maximum, lies close to point $x=x_{0}$. Then we can use the second order Taylor approximation in the neighbourhood of $x_{0}$.
\begin{equation}
f(x_{0}+h) \approx f(x_{0}) + f'(x_0)h + \frac{1}{2}f''(x_{0})h^{2}
\end{equation}

This equation in $h$ has a maximum for $h=\hat{h}$ where $x=\hat{x}=x_{0}+\hat{h}$. This maximum can be found from
\begin{equation}
\frac{d}{dh}f(x_{0}+\hat{h})=f'(x_{0})+f''(x_{0})\hat{h}=0
\end{equation}

or
\begin{equation}
\hat{h}=-\frac{f'(x_{0})}{f''(x_{0})}
\label{eq:hat_h}
\end{equation}

This does not insure that $f(x)$ reaches its maximum for $x=\hat{x}=x_{0}+\hat{h}$. This is because the second order Taylor approximation is just that, an approximation, and only valid for small values of $h$. An iterative process is needed that gradually gets us to a value of $\hat{x}$ where the value of $f'(\hat{x})$ is close enough to zero. What ''close enough" is, is a treshold that we have to define.

\section{Examples}

The examples are taken from the topic of the \emph{method of Maximum Likelihood} in statistics. There are different methods for estimating model parameters of a distribution and one of these methods uses the Likelihood Function (or its logarithm). Sometimes an analytical solution is possible, but in other cases we have to use a numerical technique such as the Newton-Raphson method.

\subsection{Example 1: Estimating the mean $\mu$ of a normal distribution with known a variance $\sigma^{2}$}

This is a classic starting point for estimating parameters in statistics textbooks.

\newthought{We can find an unbiased estimator as follows}:
\newline
We have $X \sim \mathcal{N}(\mu,\,\sigma^{2})$, with $\sigma^{2}$ known. We take a sample $(x_{1}, x_{2}, \ldots x_{n})$ of size $n$ from this distribution. Then we know that $X_{1} \sim \mathcal{N}(\mu,\,\sigma^{2})$, $X_{2} \sim \mathcal{N}(\mu,\,\sigma^{2})$, $\ldots$ $X_{n} \sim \mathcal{N}(\mu,\,\sigma^{2})$. This means that the sample distribution of the derived chance variable $\bar{X}=\frac{1}{n}(X_{1}+X_{2} + \ldots + X_{n})$ will be $\bar{X} \sim \mathcal{N}(\mu,\,\frac{\sigma^{2}}{n})$. Therefore $\bar{x}= \frac{1}{n}(x_{1}+x_{2} + \ldots + x_{n})$ will be an unbiased estimater of $\mu$ because $E(\bar{X})=E(X)=\mu$
\begin{equation}
estimate(\mu)=\bar{x}=\frac{\sum_{i=1}^{i=n}x_{i}}{n}
\label{eq:estimate_xbar}
\end{equation}

\newthought{The Method of the Maximum Likelihood} takes a different approach. Suppose we estimate that the value for $\mu$ is $\mu_1$. Knowing that $X \sim \mathcal{N}(\mu,\,\sigma^{2})$, we can calculate the probability of our first observation in the sample:
\begin{equation}
P[X=x_{1}|\mu=\mu_{1}]=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x_{1}-\mu_{1}}{\sigma})^{2}}
\end{equation}

If our estimate $\mu_{1}$ is very different from $\mu$, this probability will be very low (Figure~\ref{fig:likelihooddemo}). With a better estimate $\mu_{2}$ the probability will be higher.
<<label=likelihooddemo,fig=TRUE,include=FALSE, echo=FALSE>>=
set.seed(2018)
demo <- data.frame(t=seq(50,150,length.out = 100),y=0, y1=0, y2=0)
# setting the distribution parameters
mu <- 100
sigma <- 10
mu1 <- 70
mu2 <- 105
demo$y <- dnorm(demo$t, mu, sigma)
demo$y1 <- dnorm(demo$t, mu1, sigma)
demo$y2 <- dnorm(demo$t, mu2, sigma)
sample_x <- 95
ggplot() +
  geom_line(data=demo, aes(x=t, y=y), color="Black", linetype=2) +
  geom_line(data=demo, aes(x=t, y=y1), color="Blue", linetype=1) +
  geom_line(data=demo, aes(x=t, y=y2), color="Red", linetype=1) +
  geom_point(aes(x=sample_x, y=0), color="Black") +
  geom_point(aes(x=sample_x, y=dnorm(sample_x, mu1, sigma)), color="Blue") +
  geom_point(aes(x=sample_x, y=dnorm(sample_x, mu2, sigma)), color="Red") +
  geom_line(aes(x=c(sample_x, sample_x), y=c(0,dnorm(sample_x, mu1, sigma))),colour="Blue") +
  geom_line(aes(x=c(sample_x, sample_x), y=c(0,dnorm(sample_x, mu2, sigma))),colour="Red") +
  annotate("text", x=sample_x, y=0.003, colour="Blue", label="low probability") +
  annotate("text", x=sample_x, y=0.025, colour="Red", label="high probability") +
  geom_line(aes(x=c(mu1, mu1), y=c(0,dnorm(mu1, mu1, sigma))), color="Blue", linetype=3) +
  annotate("text", x=mu1, y=0.01, label="poor", colour="Blue") +
  annotate("text", x=mu1, y=0.008, label="estimate", colour="Blue") +
  geom_line(aes(x=c(mu2, mu2), y=c(0,dnorm(mu2, mu2, sigma))), color="Red", linetype=3) +
  annotate("text", x=mu2, y=0.01, label="better", colour="Red") +
  annotate("text", x=mu2, y=0.008, label="estimate", colour="Red") +
  labs(title = "Unknown distribution N(100,10) and esti-\nmated distributions N(70,10) and N(95,10)",
       x = "x",
       y = "probability density function") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{NR-likelihooddemo}
\caption{}
\label{fig:likelihooddemo}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

For all the other observations in the sample we can calculate their probability in the same way. Assuming that all observations are independent, the total probability of having this specific sample with this particular estimate of $\mu=\mu_{1}$, will be equal to the product of all the individual probabili\-ties. We call this the Likelihood of the sample:
\begin{equation}
\mathcal{L}((x_{1},x_{2}, \ldots x_{n})|\mu=\mu_{1})=\prod_{i=1}^{i=n}P[X=x_{i}|\mu=\mu_{1}]=\prod_{i=1}^{i=n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x_{i}-\mu_{1}}{\sigma})^{2}}
\end{equation}

With the different estimate $\mu=\mu_{2}$ we will get a different value for the Likelihood of the sample. In general: the Likelihood of the sample is a function of our estimate of $\mu$:
\begin{equation}
\mathcal{L}((x_{1},x_{2}, \ldots x_{n})|\mu)=\prod_{i=1}^{i=n}P[X=x_{i}|\mu]=\prod_{i=1}^{i=n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x_{i}-\mu}{\sigma})^{2}}
\end{equation}

Instead of working with the \emph{Likelihood function}, it is easier to work with the (natural) logarithm of this function, the \emph{Log Likelihood function}:
\begin{equation}
\ell((x_{1},x_{2}, \ldots x_{n})|\mu)=ln(\mathcal{L}(x_{1},x_{2}, \ldots x_{n})|\mu))=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) + \sum_{i=1}^{i=n} -\frac{1}{2}\left( \frac{x_{i}-\mu}{\sigma} \right) ^{2}
\end{equation}

The \emph{Likelihood function} and the \emph{Log Likelihood function} have their maximum for the same value of $\mu=\hat{\mu}$. We find $\hat{\mu}$ from
\begin{equation}
\frac{d}{d\mu}\ell((x_{1},x_{2}, \ldots x_{n})|\hat{\mu})= \sum_{i=1}^{i=n}\frac{x_{i}-\hat{\mu}}{\sigma^{2}}=0
\end{equation}
or
\begin{equation}
\hat{\mu}=\frac{\sum_{i=1}^{i=n}x_{i}}{n}=\bar{x}
\end{equation}

In this case the Method of Maximum Likelihood gives us an equation that can be solved analytically and it has the same result as in equation \ref{eq:estimate_xbar}.

\newthought{Finding the maximum of the Log Likelihood function} is not always possible in an analytical way. Then we have to use a numerical method such as Newton-Raphson. For a numerical solution we need to have a specific situation. Suppose we take a sample of size 10 from a normal distribution with $\mu=100$ and $\sigma=10$:
<<>>=
set.seed(2018)
n <- 10
mu <- 100
sigma <- 10
x <- rnorm(n,mu,sigma)
@
Then we calculate the Log Likelihood function for 100 values of $\mu$ ranging from $\mu-2\sigma$ to $\mu+2\sigma$ and make a plot this function (Figure~\ref{fig:llnorm}).
<<label=llnorm,fig=TRUE,include=FALSE, echo=TRUE>>=
# ll = Log Likelihood function of mu
ll <- data.frame(mu=seq(mu-2*sigma,mu+2*sigma,length.out = 100),log.likelihood=0)
for (i in 1:nrow(ll)) {
  ll$log.likelihood[i] <- - (1/(2*sigma^2))*sum((x-ll$mu[i])^2)
}
ll$log.likelihood <- ll$log.likelihood + n*log(sqrt(2*pi)*sigma)
ggplot(data=ll, aes(x=mu,y=log.likelihood)) +
  geom_line() +
  labs(title = "Log Likelihood of sample (size=10) from N(100,10)",
       x = expression(mu),
       y = "ll(mu)") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{NR-llnorm}
\caption{}
\label{fig:llnorm}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

The Newton-Raphson method starts with a value $\mu_{0}$. From there we calculate $\hat{h}$ so that the maximum will be found at $\hat{\mu}=\mu_{0}+\hat{h}$. Intuitively, most people would choose $\mu_{0}=\bar{x}$ (and they would be spot on from the first try!). Let us be naive and pick $\mu_{0}=min(x_{i})$. Equation \ref{eq:hat_h} gives us the value of $\hat{h}$ based on the first and second derivative of $\ell(\mu)$. The Log Likelihood function, its 1st and 2nd derivative in this case are:
\begin{equation}
\begin{split}
\ell((x_{1},x_{2}, \ldots x_{n})|\mu)&=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) - \sum_{i=1}^{i=n} \frac{1}{2}\left( \frac{x_{i}-\mu}{\sigma} \right) ^{2} \\
\frac{d}{d\mu}\ell((x_{1},x_{2}, \ldots x_{n})|\mu)&=\sum_{i=1}^{i=n}\left( \frac{x_{i}-\mu}{\sigma^{2}} \right)\\
\frac{d^{2}}{d\mu^{2}}\ell((x_{1},x_{2}, \ldots x_{n})|\mu)&= - \frac{n}{\sigma^{2}}
\end{split}
\end{equation}

We can see that the second derivative is constant and negative. This means that the extremum will be a maximum (as we can see from Figure~\ref{fig:llnorm}). We will check if the first derivative is zero for $\hat{\mu}$. It probably will not be exactly zero so we have to set a level under which the iteration will be stopped.
<<>>=
#
# Numerical solution with 2nd order Tayler approximation of the log likelihood function
#
level <- 1e-5
# Starting value
mu0 <- min(x)
#
beneath_level <- FALSE
# Second derivative is a constant
df2 <- -n/(sigma^2)
# We keep a record of the values that mu takes on during the iteration
mu.reposit <- data.frame(mu=mu0)
while(!beneath_level) {
  df1 <- sum(x-mu0)/(sigma^2)
  # Calculate mu_hat
  mu_hat <- mu0 - df1/df2
  # Add new value to the repository of mu
  mu.reposit <- rbind(mu.reposit,mu_hat)
  # Calculate the first derivative and check if it is close enough to zero (beneath the level)
  df1 <- sum(x-mu_hat)/(sigma^2)
  if (abs(df1) < level) {
    beneath_level <- TRUE
  }
  # Put the new value of mu into mu0
  mu0 <- mu_hat
}
paste("The numerical solution is: mu=",mu0)
paste("The analytical solution is: xbar=",mean(x))
mu.reposit
@

As we can see, the numerical iteration needed only one step to find the exact solution. This is because the Log Likelihood function in this case is given by:
\begin{equation}
\ell((x_{1},x_{2}, \ldots x_{n})|\mu)=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) - \sum_{i=1}^{i=n} \frac{1}{2}\left( \frac{x_{i}-\mu}{\sigma} \right) ^{2}
\end{equation}
This is a quadratic equation in $\mu$, which means that its second order Taylor approximation is exactly equal to the function itself.

\subsection{Example 2: Estimating the probability $\pi$ of a binomial experiment}

When we do an binomial experiment we have $n$ tries where each try can only have two outcomes: ''success" of "failure". Let $X$ be the chance variable that is equal to the number of successes, than this number will depend on the probability of success $\pi$ on each try. $X$ has a Binomial distribution $X \sim \mathcal{B}(\pi)$ and the probability that $X=k$ is given by:
\begin{equation}
P[X=k|\pi]={n \choose k}\pi^{k}(1-\pi)^{(n-k)}
\end{equation}

Suppose we do an experiment with $n=10$ tries. The results of the 10 tries are: $(0, 1, 1, 1, 0, 1, 1, 1, 1, 1)$. The number of successes is $X=8$. Can we make an estimate of $\pi$ based on this result?

The Likelihood for obtaining a ''0" in the first experiment is:
\begin{equation}
P[x_{1}=0|\pi]=(1-\pi)
\end{equation}

The Likelihood for obtaining each of the registred results is:
\begin{equation}
\begin{split}
P[x_{1}&=0|\pi]=(1-\pi) \\
P[x_{2}&=1|\pi]=\pi \\
P[x_{3}&=1|\pi]=\pi \\
P[x_{4}&=1|\pi]=\pi \\
P[x_{5}&=0|\pi]=(1-\pi) \\
P[x_{6}&=1|\pi]=\pi \\
P[x_{7}&=1|\pi]=\pi \\
P[x_{8}&=1|\pi]=\pi \\
P[x_{9}&=1|\pi]=\pi \\
P[x_{10}&=1|\pi]=\pi
\end{split}
\end{equation}

The Likelihood for this specific series of results is the product of the probabilities on each try:
\begin{equation}
\mathcal{L}((x_{1},x_{2}, \ldots x_{10})|\pi)=\mathcal{L}(X=k|\pi)=\prod_{i=1}^{i=10}P[x_{i}|\pi]=\pi^{8}(1-\pi)^{2}=\pi^{k}(1-\pi)^{n-k}
\end{equation}

The Log Likelihood function is:
\begin{equation}
\ell(k|\pi)=k.ln(\pi) + (n-k).ln((1-\pi))
\end{equation}

It looks like this (Figure~\ref{fig:llbinom})
<<label=llbinom,fig=TRUE,include=FALSE, echo=TRUE>>=
n <- 10
x <- c(0, 1, 1, 1, 0, 1, 1, 1, 1, 1)
k <- sum(x)
# ll = Log Likelihood function of pi
ll <- data.frame(p=seq(0.01, 0.99, length.out = 100),log.likelihood=0)
ll$log.likelihood <- k*log(ll$p) + (n - k)*log(1-ll$p)
ggplot(data=ll, aes(x=p,y=log.likelihood)) +
  geom_line() +
  labs(title = "Log Likelihood of binomial experiment",
       x = expression(pi),
       y = "ll(pi)") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{NR-llbinom}
\caption{}
\label{fig:llbinom}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

The Log Likelihood function and the first and second derivatives are:
\begin{equation}
\begin{split}
\ell(k|\pi) &=k.ln(\pi) + (n-k).ln(1-\pi) \\
\frac{d}{d\pi}\ell(k)|\pi) &=\frac{k}{\pi} - \frac{n-k}{1-\pi} \\
\frac{d^{2}}{d\pi^{2}}\ell(k|\pi) &=-\frac{k}{\pi^{2}} -  \frac{n-k}{(1-\pi)^{2}}
\end{split}
\end{equation}

Again, the search for the $\pi$-value for which the Log Likelihood function reaches a maximum (we can see that the second derivative is always negative), has an analytical solution because $\frac{d}{d\pi}\ell(k|\pi)=\frac{k}{\pi} -  \frac{n-k}{1-\pi}=0$ for $\pi=\frac{k}{n}=0.8$.

We can find this also numerically, but it will not be so easy because the Log Likelihood function is clearly not quadratic.

<<>>=
#
# Numerical solution with 2nd order Tayler approximation of the log likelihood function
#
level <- 1e-5
# Starting value
p0 <- min(ll$p)
#
beneath_level <- FALSE
# We keep a record of the values that p takes on during the iteration
p.reposit <- data.frame(p=p0)
while(!beneath_level) {
  df1 <- k/p0 - (n - k)/(1 - p0)
  df2 <- -k/(p0^2) - (n - k)/((1 - p0)^2)
  # Calculate p_hat
  p_hat <- p0 - df1/df2
  # Add new value to the repository of p
  p.reposit <- rbind(p.reposit,p_hat)
  # Calculate the first derivative and check if it is close enough to zero (beneath the level)
  df1 <- k/p_hat - (n - k)/(1 - p_hat)
  if (abs(df1) < level) {
    beneath_level <- TRUE
  }
  # Put the new value of p into p0
  p0 <- p_hat
}
paste("The numerical solution is: pi=",p0)
paste("The analytical solution is: pi=",k/n)
p.reposit
@

We find the correct result, but the iteration takes much longer because the Taylor approximation in this case is truly an approximation!

\chapter{Function of two variables}
\section{Taylor approximation}
Taylor's Theorem says that if a function $f(x,y)$ is at least two times differentiable on an open interval I, than for any two points $(x,y)$ and $(x+h_{x}, y+h_{y})$ ($h_{x}$ and $h_{y}$ small) the second order Taylor approximation is given by:

\begin{equation}
f(x+h_{x},y+h_{y}) \approx f(x,y) + \frac{\partial}{\partial x}f(x,y).h_{x} + \frac{\partial}{\partial y}f(x,y).h_{y} +\frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}f(x,y).h_{x}^{2} + \frac{\partial^{2}}{\partial x \partial y}f(x,y).h_{x}.h_{y} +\frac{1}{2}\frac{\partial^{2}}{\partial y^{2}}f(x,y).h_{y}^{2}
\end{equation}

\section{Finding the maximum of a function of two variables}

A function $f(x,y)$ reaches a (local) maximum for $(x=\hat{x},y=\hat{y}$ when
\begin{equation}
\begin{cases}
  \frac{\partial}{\partial x}f(\hat{x},\hat{y})=0 \\
  \frac{\partial}{\partial y}f(\hat{x},\hat{y})=0
\end{cases}
\end{equation}

Suppose that ($\hat{x}, \hat{y}$) are the values of $x$ and $y$ where $f(x,y)$ reaches a maximum. If ($\hat{x}, \hat{y}$) is close to ($x_{0},y_{0}$), then we can use the second order Taylor approximation in the neighbourhood of ($x_{0},y_{0}$):
\begin{equation}
\begin{split}
f(x_{0}+h_{x},y_{0}+h_{y}) \approx f(x_{0},y_{0}) & + \frac{\partial}{\partial x}f(x_{0},y_{0}).h_{x} + \frac{\partial}{\partial y}f(x_{0},y_{0}).h_{y}\\
& +\frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}f(x_{0},y_{0}).h_{x}^{2} + \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{x}.h_{y} +\frac{1}{2}\frac{\partial^{2}}{\partial y^{2}}f(x_{0},y_{0}).h_{y}^{2}
\end{split}
\end{equation}

The values $\hat{h_{x}}$ and  $\hat{h_{y}}$ for which $\hat{x}=x_{0}+\hat{h_{x}}$ and $\hat{y}=y_{0}+\hat{h_{y}}$ can then be found from the following set of equations:
\begin{equation}
  \begin{cases}
    \frac{\partial}{\partial h_{x}}f(x_{0}+\hat{x},y_{0}+\hat{y})= \frac{\partial}{\partial x}f(x_{0},y_{0}) + \frac{\partial^{2}}{\partial x^{2}}f(x_{0},y_{0}).h_{x} + \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{y}=0 \\
    \frac{\partial}{\partial h_{y}}f(x_{0}+\hat{x},y_{0}+\hat{y})= \frac{\partial}{\partial y}f(x_{0},y_{0}) + \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{x} + \frac{\partial^{2}}{\partial y^{2}}f(x_{0},y_{0}).h_{y}=0
  \end{cases}
\end{equation}

or
\begin{equation}
  \begin{cases}
    \frac{\partial^{2}}{\partial x^{2}}f(x_{0},y_{0}).h_{x} + \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{y}= - \frac{\partial}{\partial x}f(x_{0},y_{0}) \\
    \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}).h_{x} + \frac{\partial^{2}}{\partial y^{2}}f(x_{0},y_{0}).h_{y}= -\frac{\partial}{\partial y}f(x_{0},y_{0})
  \end{cases}
\label{eq:set}
\end{equation}

We can solve this set of equations quickly in R using matrices. We can write the set of equations \ref{eq:set} as follows:
\begin{equation}
\textbf{A} \boldsymbol{\cdot} \textbf{h} =\textbf{b}
\end{equation}

with
\begin{equation}
  \textbf{A}= \begin{bmatrix} \frac{\partial^{2}}{\partial x^{2}}f(x_{0},y_{0}) & \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}) \\ \frac{\partial^{2}}{\partial x \partial y}f(x_{0},y_{0}) & \frac{\partial^{2}}{\partial y^{2}}f(x_{0},y_{0}) \end{bmatrix} = \textbf{Hess}(f(x_{0},y_{0}))
\end{equation}
the \emph{Hessian} matrix of $f(x,y)$ in $(x_{0},y_{0})$

and
\begin{equation}
  \textbf{b}= \begin{bmatrix} - \frac{\partial}{\partial x}f(x_{0},y_{0}) \\ - \frac{\partial}{\partial y}f(x_{0},y_{0}) \end{bmatrix} = - \boldsymbol{\nabla}(f(x_{0},y_{0}))
\end{equation}
with $\boldsymbol{\nabla}(f(x_{0},y_{0}))$ \emph{gradient} vector of $f(x,y)$ in $(x_{0},y_{0})$

and
\begin{equation}
  \textbf{h}= \begin{bmatrix} h_{x} \\ h_{y} \end{bmatrix}
\end{equation}
the vector of the values we are trying to find.

Again, this will not insure that $(\hat{x}=x_{0}+\hat{h_{x}},\hat{y}=y_{0}+\hat{h_{y}})$ will be where $f(x,y)$ reaches its maximum. The second order Taylor approximation remains an approximation, and is only valid for small values of $(h_{x},h_{y})$. In the iterative process we will check if $\frac{\partial}{\partial x}f(\hat{x},\hat{y}) and \frac{\partial}{\partial y}f(\hat{x},\hat{y})$ are close enough to zero. We will use the RMS value to test this condition:
\begin{equation}
RMS = \sqrt{\frac{\partial}{\partial x}f(\hat{x},\hat{y})^{2}+\frac{\partial}{\partial y}f(\hat{x},\hat{y})^{2}}
\end{equation}

\section{Examples}

\subsection{Example 1: Estimating the coefficients of a linear regression model}

Choosing a linear regression model means that we assume that the real relation between a response variable $Y$ and an independent variable $X$ has the form:
\begin{equation}
Y = \beta_{0}+\beta_{1}X
\end{equation}

However, when we take $n$ measurements of couples $(x_{i},y_{i})$ these, in general, do not lie on a straigth line. This is considered to be caused by other unknown or uncontrollable variables. However we assume that the net effect of these disturbances is normally distributed $U \sim \mathcal{N}(0,\sigma^{2})$, and that the value of $\sigma$ is independent of $X$\sidenote{LineaireRegressie.pdf}:
\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}X_{i}+U_{i}
\end{equation}
Because $\beta_{0}$, $\beta_{1}$ are constants and $X_{i}$ are known values this implies that $Y_{i}$ is normally distributed: $Y_{i} \sim \mathcal{N}(\beta_{0}+\beta_{1}X_{i},\sigma^{2})$

\newthought{The classical method} to estimate $\beta_{0}$ and $\beta_{1}$ is the \emph{Ordinary Least Squares Method}. It minimises $Q$, the sum of the square of the differences between the observed value of $Y$ and the calculated value $\hat{Y}$. 
\begin{equation}
Q=\sum\limits_{i=1}^{i=n}\left( Y_{i}-\beta_{0}-\beta_{1}x_{i}  \right)^{2}
\end{equation}

The minimum is calculated by taking the partial derivatives of $Q$ with respect to $\beta_0$ and $\beta_{1}$, setting these equal to 0, and solving this set of two equations. These two equations are called the \emph{normal equations} \sidenote{Because the calculation is based on one set of $n$ observations, we cannot find $\beta_{0}$ and $\beta_{1}$, but estimates $b_{0}$ and $b_{1}$ based on this sample}:
\begin{equation}
\begin{cases}
nb_{0}+b_{1}\sum_{i=1}^{i=n}x_{i}&=\sum_{i=1}^{i=n}y_{i} \\
b_{0}\sum_{i=1}^{i=n}x_{i}+b_{1}\sum_{i=1}^{i=n}x_{i}^{2}&=\sum_{i=1}^{i=n}x_{i}y_{i}
\end{cases}
\label{eq:normaleq}
\end{equation}

The solutions of the \emph{normal equations} are:
\begin{equation}
\begin{split}
b_{0}&=\bar{y}-b_{1}\bar{x}\\
b_{1}&=\frac{\sum\limits_{i=1}^{i=n} (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum\limits_{i=1}^{i=n}(x_{i}-\bar{x})^{2}}
\end{split}
\end{equation}

The \emph{Gauss-Markov Theorem} proves that the estimates for $\beta_{0}$ and $\beta_{1}$ obtained by the \emph{Ordinary Least Squares Method} are unbiased and have the smallest variance compared with other estimating methods.

\newthought{The Maximum Likelihood Method} uses the assumption that $Y_{i} \sim \mathcal{N}(\beta_{0}+\beta_{1}X_{i},\sigma^{2})$ to calculate the probability of observing the value $y_{i}$ when we use $\beta_{0}=\beta_{0}^{*}$ and $\beta_{1}=\beta_{1}^{*}$ as estimates for $\beta_{0}$ and $\beta_{1}$:
\begin{equation}
P[Y=y_{i}|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}{*}]=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{y_{i}-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}{\sigma})^{2}}
\end{equation}

The \emph{Likelihood function} for the whole sample is:
\begin{equation}
\mathcal{L}((y_{1},y_{2}, \ldots y_{n})|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}^{*})=\prod_{i=1}^{i=n}P[Y=y_{i}|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}^{*}]=\prod_{i=1}^{i=n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{y_{1}-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}{\sigma})^{2}}
\end{equation}

and the \emph{Log Likelihood function} is:
\begin{equation}
\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}^{*})=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) + \sum_{i=1}^{i=n} -\frac{1}{2}\left( \frac{y_{1}-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}{\sigma} \right) ^{2}
\end{equation}

Other estimates for $\beta_{0}$ and $\beta_{1}$ will give other values for $\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})$. In short: the \emph{Log Likelihood function} is a function of $\beta_{0}$ and $\beta_{1}$. The maximum of the \emph{Log Likelihood function} is found from the system of equations that we find by setting the partial derivatives of $\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})$ with respect to $\beta_{0}$ and $\beta_{1}$ equal to zero:
\begin{equation}
\begin{cases}
\frac{\partial}{\partial \beta_{0}}\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})=\frac{1}{\sigma^{2}}\sum_{i=1}^{i=n}\left( y_{i}-\beta_{0}-\beta_{1}x_{i} \right)=0 \\
\frac{\partial}{\partial \beta_{1}}\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})=\frac{1}{\sigma^{2}}\sum_{i=1}^{i=n}\left( y_{i}-\beta_{0}-\beta_{1}x_{i} \right)x_{i}=0
\end{cases}
\end{equation}

This is identical to the set of \emph{normal equations} (\ref{eq:normaleq}) that we found using the \emph{Ordinary Least Squares method}. The \emph{Maximum Likelihood method} in this case has an analytical solution, and it is the same as the one found by the \emph{Ordinary Least Squares method}.
\medskip

We can of course also use the numerical \emph{Newton-Raphson method} to find the maximum. The \emph{Log Likelihood function} is:
\begin{equation}
\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0},\beta_{1})=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) + \sum_{i=1}^{i=n} -\frac{1}{2}\left( \frac{y_{1}-\beta_{0}-\beta_{1}x_{i}}{\sigma} \right) ^{2}
\end{equation}

The function is quadratic in $\beta_{0}$ and $\beta_{1}$, so the use of the second order Taylor approximation should give a solution after one iteration.

We start by generating a sample of $n$ observations of the form $y_{i}=\beta_{0}+\beta_{1}x_{i}+u_{i}$ where $U \sim \mathcal{N}(0,\sigma^{2})$ (Figure~\ref{fig:linear}).
\medskip
<<label=linear,fig=TRUE,include=FALSE, echo=TRUE>>=
set.seed(2018)
# Defining beta0, beta1 and sigma
beta0 <- 1
beta1 <- 2
sigma <- 1
# Creating the sample
n <- 10
x <- seq(1:n)
y <- beta0 + beta1*x + rnorm(n,0,sigma)
# Graph of (x,y)
df <- data.frame(x=x,y=y)
ggplot(data=df, aes(x=x,y=y)) +
  geom_point() +
  labs(title = "Observations",
       x = "x",
       y = "y") +
  JT.theme
@

\begin{marginfigure}[-3cm]
\includegraphics[width=1\textwidth]{NR-linear}
\caption{}
\label{fig:linear}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}
\medskip
The \emph{Log Likelihood function} as a function of $\beta_{0}$ and $\beta_{1}$ looks like this (Figure~\ref{fig:lllinear})
\medskip
<<label=lllinear,fig=TRUE,include=FALSE, echo=TRUE>>=
# Graph of log likelihood function as a function of betaO and beta1
k <- 25
b0 <- seq(beta0-5*abs(beta0),beta0+5*abs(beta0),length.out = k)
b1 <- seq(-2*beta1,2*beta1,length.out = k)
ll1 <- matrix(0, nrow=k, ncol=k)
for(i in (1:k)) {
  for (j in (1:k)) {
    ll1[i,j] <- sum(y - b0[i] - b1[j]*x)^2
  }
}
ll <- n*log(1/(sqrt(2*pi)*sigma)) - (1/(2*sigma^2))*ll1
persp(b0,b1,ll,theta=135,
      xlim=c(beta0-5.5*abs(beta0),beta0+5.5*abs(beta0)),
      ylim=c(-2.5*beta1,2.5*beta1),
      zlim=c(min(ll),10000),
      xlab="beta0",
      ylab="beta1",
      main="log likelihood function",
      col="lightblue",
      shade=0.25,
      ticktype="detailed")
@

\begin{marginfigure}[-10cm]
\includegraphics[width=1\textwidth]{NR-lllinear}
\caption{}
\label{fig:lllinear}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}
\medskip
To find the maximum of the \emph{Log Likelihood function} we need to calculate the \emph{gradient vector} and the \emph{Hessian matrix} of the function in a chosen point $(\beta_{0}^{*},\beta_{1}^{*})$:
\begin{equation}
\begin{split}
\boldsymbol{\nabla}(\ell(\beta_{0}^{*},\beta_{1}^{*})) &= \begin{bmatrix} \frac{\partial}{\partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*})) \\ \frac{\partial}{\partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*})) \end{bmatrix} \\
  \textbf{Hess}(\ell(\beta_{0}^{*},\beta_{1}^{*})) &= \begin{bmatrix} \frac{\partial^{2}}{\partial \beta_{0}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*}) & \frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*}) \\ \frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*}) & \frac{\partial^{2}}{\partial \beta_{1}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*}) \end{bmatrix}
\end{split}
\end{equation}

The elements of the \emph{gradient vector} are:
\begin{equation}
\begin{split}
\frac{\partial}{\partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=\frac{1}{\sigma^{2}}\sum_{i=1}^{i=n}(y_{i}-\beta_{0}^{*}-\beta_{1}^{*}x_{i}) \\
\frac{\partial}{\partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=\frac{1}{\sigma^{2}}\sum_{i=1}^{i=n}(y_{i}-\beta_{0}^{*}-\beta_{1}^{*}x_{i})x_{i}
\end{split}
\end{equation}

The elements of the \emph{Hessian matrix} are:
\begin{equation}
\begin{split}
\frac{\partial^{2}}{\partial \beta_{0}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=-\frac{n}{\sigma^{2}} \\
\frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*})=\frac{\partial^{2}}{\partial \beta_{1} \partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=-\frac{n}{\sigma^{2}}\sum_{i=1}^{i=n}x_{i} \\
\frac{\partial^{2}}{\partial \beta_{1}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=-\frac{n}{\sigma^{2}}\sum_{i=1}^{i=n}x_{i}^{2}
\end{split}
\end{equation}

All we need now to start the iteration are initial values $\beta_{0}^{*}$ and $\beta_{1}^{*}$. A possible choice is to use the line  defined by the couples $(x_{min},y(x_{min}))$ and $(x_{max},y(x_{max}))$:
\begin{equation}
\frac{y-y(x_{min})}{x-x_{min}}=\frac{y(x_{max})-y(x_{min})}{x_{max}-x_{min}}
\end{equation}

Setting $\beta_{1}^{*}=\frac{y(x_{max})-y(x_{min})}{x_{max}-x_{min}}$ we find $\beta_{0}^{*}=y(x_{min}-\beta_1)^{*}x_{min}$.
\medskip

The iteration:

\medskip
<<>>=
# Level beneath which the RMS first derivative is considered to be equal to zero
level <- 1e-5
# Initialising beta0 and beta1: line through the extreme points
dfsorted <- df[order(x),]
b1star <- (dfsorted$y[n] - dfsorted$y[1])/(dfsorted$x[n] - dfsorted$x[1])
b0star <- dfsorted$y[1] - b1star*dfsorted$x[1]
beneath_level <- FALSE
# We keep a record of the values that b0 and b1 take during the iteration
beta.reposit <- data.frame(b0=b0star,b1=b1star)
while(!beneath_level) {
  grad.ll <- matrix(c((1/sigma^2)*sum(y-b0star-b1star*x),(1/sigma^2)*sum((y-b0star-b1star*x)*x)),nrow=2,ncol=1)
  Hess.ll <- -(1/sigma^2)*matrix(c(n,sum(x),sum(x),sum(x^2)),nrow=2,ncol=2)
  # Calculate b0_hat en b1_hat
  h_hat <- solve(Hess.ll,-grad.ll)
  b0_hat <- b0star + h_hat[1]
  b1_hat <- b1star + h_hat[2]
  # Add new value to the repository of b0 and b1
  beta.reposit <- rbind(beta.reposit,c(b0_hat,b1_hat))
  # Calculate the first derivative and check if it is close enough to zero (beneath the level)
  grad.ll <- matrix(c((1/sigma^2)*sum(y-b0_hat-b1_hat*x),(1/sigma^2)*sum((y-b0_hat-b1_hat*x)*x)),nrow=2,ncol=1)
  if (sqrt(grad.ll[1]^2 + grad.ll[2]^2) < level) {
    beneath_level <- TRUE
  }
  # Put the new value of b0 and b1 into b0star and b1star
  b0star <- b0_hat
  b1star <- b1_hat
}
paste("The numerical solutions are: b0=",b0star," b1=",b1star)
# The Ordinary Least Squares solution
A <- matrix(c(n,sum(x),sum(x),sum(x^2)), nrow=2, ncol=2)
b <- matrix(c(sum(y),sum(x*y)), nrow=2, ncol=1)
paste("The results of the linear regression with OLS-method are:")
paste("b0=",solve(A,b)[1]," and b1=",solve(A,b)[2])
beta.reposit
@
\medskip
We can see that our initial estimate was not bad, and that in one iteration step we numerically found the same result as the \emph{Ordinary Least Squares method}. The values of $b_{0}$ and $b_{1}$ are of course not the same as $\beta_{0}$ and $\beta_{1}$ that we used to create the sample, because this sample is only one of many due to the (normal) error.

\subsection{Example 2: Estimating the coefficients of a logistic regression model}

We use logistic regression when the response variable $Y$ takes on only two values like (TRUE, FALSE), (Sick, Healthy)\sidenote{coded as ''1" and ''0"} ... as a function of a continuous independent variable $X$ (or continuous variables $X_{i}$). Graphically we get something like Figure~\ref{fig:scatterlogistic}.

<<label=scatterlogistic,fig=TRUE,include=FALSE, echo=FALSE>>=
dflogistic <- data.frame(x=c(12, 13, 14, 17, 18, 19, 19, 19, 20, 20.5, 21, 21, 21, 21, 22, 23, 24, 24, 25, 25, 25.5, 26, 27, 27.2), y=c(0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1))
n <- nrow(dflogistic)
ggplot(data=dflogistic) + 
  geom_point(aes(x=x, y=y)) +
    labs(title="Observations", 
         xlab="x", 
         ylab="y: 1=yes, 0=no") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{NR-scatterlogistic}
\caption{Plot of a data for logistic regression}
\label{fig:scatterlogistic}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

We can see that for larger values of $X$ the value of $Y$ tends to be ''1", while for lower values of $X$, $Y$ is predominantly ''0". Because $Y_{i}$ can only take on two values (''0" and ''1") we can look at this situation as a binomial experiment, where the probability that $Y_{i}=1$ is equal to $\pi_{i}$.
\begin{equation}
P[Y_{i}=1|X=x_{i}]=\pi_{i}
\end{equation}

Instead of trying to predict the value of $Y$ for a given value of $X$, the logistic regression predicts the probability $\pi_{i}$. This seems odd, but we should recall that, for example, a linear regression also does not give an exact formula to calculate $Y$ for $X=x_{i}$ but gives a confidence interval for the predicted value.

In Figure~\ref{fig:scatterlogistic} we see that $P[Y_{i}=1]=\pi_{i}$ increases with $x$. A simple model could be a linear relationship:
\begin{equation}
\pi_{i}=\beta_{0}+\beta_{1}x_{i}
\end{equation}

This is really too simple because it can easily lead to values of $\pi_{i}$ that are either negative or greater than 1, which is impossible for a probability. There are other reasons as well why a linear regression is not appropriate. A better relation between $\pi_{i}$ and $x_{i}$ is the \emph{logit function}\sidenote{See Logistic Regression}:
\begin{equation}
\pi_{i}=\frac{e^{\beta_0 + \beta_{1}x_{i}}}{1+e^{\beta_0 + \beta_{1}x_{i}}}=\frac{1}{1+e^{-\beta_0 - \beta_{1}x_{i}}}
\label{eq:logit}
\end{equation}

The probability that $Y$ takes on the value $y_{i}=1$ or $y_{i}=0$ when $X=x_{i}$ can be written as:
\begin{equation}
\begin{split}
P[Y=y_{i}=1|X=x_{i}]&=\pi_{i} \\
P[Y=y_{i}=0|X=x_{i}]&=1-\pi_{i}
\end{split}
\end{equation}

We can write this in one line:
\begin{equation}
P[Y=y_{i}|X=x_{i}]=\pi_{i}^{y_{i}}(1-\pi_{i})^{1-y_{i}}
\end{equation}

because for $y_{i}=1$ it gives $P[Y=y_{i}|X=x_{i}]=\pi_{i}$, and for $y_{i}=0$ it gives $P[Y=y_{i}|X=x_{i}]=(1-\pi_{i})$.

The probability that we get this specific sample of $n$ observations is the \emph{Likelihood function} for this sample:
\begin{equation}
\mathcal{L}((y_{1},y_{2}, \ldots y_{n})|(\pi_{1},\pi_{2}, \ldots \pi_{n}))=\prod_{i=1}^{i=n}\pi_{i}^{y_{i}}(1-\pi_{i})^{(1-y_{i})}
\end{equation}

The \emph{Log Likelihood function} is then:
\begin{equation}
\begin{split}
\ell((y_{1},y_{2}, \ldots y_{n})|(\pi_{1},\pi_{2}, \ldots \pi_{n}))&=ln \left( \mathcal{L}((y_{1},y_{2}, \ldots y_{n})|(\pi_{1},\pi_{2}, \ldots \pi_{n})) \right) \\
&=\sum_{i=1}^{i=n}y_{i}ln(\pi_{i}) + \sum_{i=1}^{i=n}(1-y_{i})ln ( 1-\pi_{i} ) \\
&= \sum_{i=1}^{i=n}y_{i} ln \frac{\pi_{i}}{1-\pi{i}} + \sum_{i=1}^{i=n}ln ( 1-\pi_{i} )
\end{split}
\end{equation}

When we use the \emph{logit function} to define the relationship between $\pi_{i}$ and $x_{i}$ we get
\begin{equation}
\begin{split}
\pi_{i}&=\frac{e^{\beta_0 + \beta_{1}x_{i}}}{1+e^{\beta_0 + \beta_{1}x_{i}}}\\
1-\pi_{i}&=\frac{1}{1+e^{\beta_0 + \beta_{1}x_{i}}} \\
\frac{\pi_{i}}{1-\pi_{i}}&=e^{\beta_0 + \beta_{1}x_{i}}
\end{split}
\end{equation}

We can write the \emph{Log Likelihood function} as a function of two varia\-bles: the parameters $\beta_{0}$ and $\beta_{1}$ of the logistic model:
\begin{equation}
\ell(\beta_{0}, \beta_{1})=\sum_{i=1}^{i=n}y_{i}(\beta_{0}+\beta_{1}x_{i}) - \sum_{i=1}^{i=n}ln(1+e^{\beta_{0}+\beta_{1}x_{i}})
\end{equation}

<<label=lllogistic,fig=TRUE,include=FALSE, echo=TRUE>>=
# Graph of log likelihood function as a function of betaO and beta1
x <- dflogistic$x
y <- dflogistic$y
k <- 50
b0 <- seq(-20,0,length.out = k)
b1 <- seq(-1,1,length.out = k)
ll <- matrix(0, nrow=k, ncol=k)
for(i in (1:k)) {
  for (j in (1:k)) {
    ll[i,j] <- sum(y*(b0[i]+b1[j]*x)) - sum(log(1+exp(b0[i]+b1[j]*x)))
  }
}
persp(b0,b1,ll,theta=135,
      xlim=c(-21,1),
      ylim=c(-1.5,1.5),
      zlim=c(-500,0),
      xlab="beta0",
      ylab="beta1",
      main="log likelihood function",
      col="lightblue",
      shade=0.25,
      ticktype="detailed")
@

\begin{marginfigure}[-4cm]
\includegraphics[width=1\textwidth]{NR-lllogistic}
\caption{}
\label{fig:lllogistic}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}
\medskip
For finding the maximum of this function there exists no analytical solution. So we use the alternative numerical method We find the values of $h_{\beta_{0}}$ and $h_{\beta_{1}}$ that will give us the values of $\beta_{0}=\beta_{0}^{*}+h_{\beta_{0}}$ and $\beta_{1}=\beta_{1}^{*}+h_{\beta_{1}}$ ($\beta_{0}^{*}$ and $\beta_{1}^{*}$ being appropriately chosen starting values) where $\ell(\beta_{0}, \beta_{1})$ reaches a maximum.
\begin{equation}
\textbf{Hess}(\ell(\beta_{0}^{*},\beta_{1}^{*})) \boldsymbol{\cdot} \textbf{h} = - \boldsymbol{\nabla}(\ell(\beta_{0}^{*},\beta_{1}^{*}))
\end{equation}
with
\begin{equation}
\begin{split}
\boldsymbol{\nabla}(\ell(\beta_{0}^{*},\beta_{1}^{*})) &= \begin{bmatrix} \frac{\partial}{\partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*})) \\ \frac{\partial}{\partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*})) \end{bmatrix} \\
  \textbf{Hess}(\ell(\beta_{0}^{*},\beta_{1}^{*})) &= \begin{bmatrix} \frac{\partial^{2}}{\partial \beta_{0}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*}) & \frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*}) \\ \frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*}) & \frac{\partial^{2}}{\partial \beta_{1}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*}) \end{bmatrix}
\end{split}
\end{equation}

The elements of the \emph{gradient vector} are:
\begin{equation}
\begin{split}
\frac{\partial}{\partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=\sum_{i=1}^{i=n}y_{i}- \sum_{i=1}^{i=n} \frac{e^{\beta_{0}^{*}+\beta_{1}^{*}x_{i}}}{1+e^{\beta_{0}^{*}+\beta_{1}^{*}x_{i}}} \\
&= \sum_{i=1}^{i=n}y_{i}- \sum_{i=1}^{i=n} \frac{1}{1+e^{-\beta_{0}^{*} - \beta_{1}^{*}x_{i}}} \\
\frac{\partial}{\partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*})&= \sum_{i=1}^{i=n}y_{i}x_{i}- \sum_{i=1}^{i=n} \frac{e^{\beta_{0}^{*}+\beta_{1}^{*}x_{i}}}{1+e^{\beta_{0}^{*}+\beta_{1}^{*}x_{i}}}x_{i}\\
&= \sum_{i=1}^{i=n}y_{i}x_{i}- \sum_{i=1}^{i=n} \frac{1}{1+e^{-\beta_{0}^{*} - \beta_{1}^{*}x_{i}}}x_{i}
\end{split}
\end{equation}

The elements of the \emph{Hessian matrix} are:
\begin{equation}
\begin{split}
\frac{\partial^{2}}{\partial \beta_{0}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=-\sum_{i=1}^{i=n}\frac{e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}}{\left( 1 + e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}} \right)^{2}} \\
\frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*})=\frac{\partial^{2}}{\partial \beta_{1} \partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=-\sum_{i=1}^{i=n}\frac{e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}}{\left( 1 + e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}} \right)^{2}}x_{i} \\
\frac{\partial^{2}}{\partial \beta_{1}^{2}}\ell(\beta_{0}^{*},\beta_{1}^{*})&=-\sum_{i=1}^{i=n}\frac{e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}}{\left( 1 + e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}} \right)^{2}}x_{i}^{2}
\end{split}
\end{equation}

\medskip
We need starting values $\beta_{0}^{*}$ and $\beta_{1}^{*}$. We could start with the values that come from the linear regression:
\medskip

<<>>=
b0star <- summary(lm(y~x))$coefficients[1]
b1star <- summary(lm(y~x))$coefficients[2]
@

\medskip

The iteration:

<<>>=
# Level beneath which the RMS first derivative is considered to be equal to zero
level <- 1e-5
beneath_level <- FALSE
# We keep a record of the values that b0 and b1 take during the iteration
beta.reposit <- data.frame(b0=b0star,b1=b1star)
while(!beneath_level) {
  grad.ll <- matrix(c(sum(y)-sum(1/(1+exp(-b0star-b1star*x))),sum(y*x)-sum(1/(1+exp(-b0star-b1star*x))*x)),nrow=2,ncol=1)
  Hess.ll <- -matrix(c(sum(exp(-b0star-b1star*x)/(1+exp(-b0star-b1star*x))^2), sum((exp(-b0star-b1star*x)/(1+exp(-b0star-b1star*x))^2)*x), sum((exp(-b0star-b1star*x)/(1+exp(-b0star-b1star*x))^2)*x), sum((exp(-b0star-b1star*x)/(1+exp(-b0star-b1star*x))^2)*x^2)) ,nrow=2,ncol=2)
  # Calculate b0_hat en b1_hat
  h_hat <- solve(Hess.ll,-grad.ll)
  b0_hat <- b0star + h_hat[1]
  b1_hat <- b1star + h_hat[2]
  # Add new value to the repository of b0 and b1
  beta.reposit <- rbind(beta.reposit,c(b0_hat,b1_hat))
  # Calculate the first derivative and check if it is close enough to zero (beneath the level)
  grad.ll <- matrix(c(sum(y)-sum(1/(1+exp(-b0_hat-b1_hat*x))),sum(y*x)-sum(1/(1+exp(-b0_hat-b1_hat*x))*x)),nrow=2,ncol=1)
  if (sqrt(grad.ll[1]^2 + grad.ll[2]^2) < level) {
    beneath_level <- TRUE
  }
  # Put the new value of b0 and b1 into b0star and b1star
  b0star <- b0_hat
  b1star <- b1_hat
}
paste("The numerical solutions are: b0=",b0star," b1=",b1star)
paste("The results of the logistic regression with R are:")
paste("b0=",summary(glm(y~x,binomial))$coefficients[1,1]," and b1=",summary(glm(y~x,binomial))$coefficients[2,1])
beta.reposit
@

\medskip
It takes 6 iterations to find the maximum of $\ell(\beta_{0},\beta_{1})$ but the results are the same as those that are given by the inbuilt logistic regression function in R \textit{glm()}:
<<>>=
summary(glm(y~x,family=binomial))
@

\medskip
Some clever use of matrices\sidenote{"Create your Machine Learning library from scratch with R! (1/5) - Linear and logistic regression", Enhance Data Science, \url{http://enhancedatascience.com/2018/01/30/your-own-machine-learning-library-from-scratch-with-r/}} can speed up the calculation.

We create the following matrices:
\begin{equation}
\begin{split}
\boldsymbol{\beta}&= \begin{bmatrix} \beta_{0}^{*} \\ \beta_{1}^{*} \end{bmatrix} \\
\textbf{x}&= \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} \\
\textbf{X}&= \begin{bmatrix} 1 & x_{1} \\ 1 & x_{2} \\ \vdots & \vdots \\ 1 & x_{n} \end{bmatrix} \\
\textbf{y}&= \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix}
\end{split}
\end{equation}

then:
\begin{equation}
\boldsymbol{\beta}^{T} \boldsymbol{\cdot} \textbf{X}^{T} = \begin{bmatrix} \beta_{0}^{*} + \beta_{1}^{*}x_{1} & \beta_{0}^{*} + \beta_{1}^{*}x_{2} & \hdots & \beta_{0}^{*} + \beta_{1}^{*}x_{n} \end{bmatrix}
\end{equation}

and
\begin{equation}
\textbf{N} = \frac{1}{1+e^{(-\boldsymbol{\beta}^{T} \boldsymbol{\cdot} \textbf{X}^{T})}} = \begin{bmatrix} \frac{1}{1+e^{-\beta_{0}^{*} - \beta_{1}^{*}x_{1}}} & \frac{1}{1+e^{-\beta_{0}^{*} - \beta_{1}^{*}x_{2}}} & \hdots & \frac{1}{1+e^{-\beta_{0}^{*} - \beta_{1}^{*}x_{n}}} \end{bmatrix}
\end{equation}

With this matrix \textbf{N} we can do the following, element by element, multiplication:
\begin{equation}
\textbf{N}\textbf{(1-N)}^{T} = \begin{bmatrix} \sum_{i=1}^{i=n} \frac{e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}}{(1 + e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{1}})^{2}} & \sum_{i=1}^{i=n} \frac{e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}}{(1 + e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{2}})^{2}} & \hdots & \sum_{i=1}^{i=n} \frac{e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{i}}}{(1 + e^{-\beta_{0}^{*}-\beta_{1}^{*}x_{n}})^{2}}  \end{bmatrix}
\end{equation}

With the R-command \textit{diag(z)} we can create the diagnonal matrix:
\begin{equation}
diag(z) = \begin{bmatrix} z_{1} & 0 & \hdots & 0 \\ 0 & z_{2} & \hdots & 0 \\ \vdots & \vdots &\hdots &\vdots \\ 0 & 0 & \hdots & z_{n} \end{bmatrix}
\end{equation}

and this helps us to calculate
\begin{equation}
\begin{split}
\boldsymbol{\nabla}(\ell(\beta_{0}^{*},\beta_{1}^{*})) &= \textbf{X}^{T} \boldsymbol{\cdot} \left( \textbf{y} - \textbf{N}^{T} \right) \\
\textbf{Hess}(\ell(\beta_{0}^{*},\beta_{1}^{*})) &= - \textbf{X}^{T} \boldsymbol{\cdot} diag((\textbf{N}\textbf{(1-N)}^{T})^{T}[,1]) \boldsymbol{\cdot} \textbf{X}
\end{split}
\end{equation}

The iteration reworked with matrices
<<>>=
# Defining the matrices
# beta with starting values derived from simple linear regression
beta.mat <- matrix(c(summary(lm(y~x))$coefficients[1],summary(lm(y~x))$coefficients[2]), nrow=2, ncol=1)
# other matrices
x.mat <- as.matrix(x)
X.mat <- matrix(c(rep(1,n),x), nrow=n, ncol=2)
y.mat <- as.matrix(y)
# Level beneath which the RMS first derivative is considered to be equal to zero
level <- 1e-5
beneath_level <- FALSE
# We keep a record of the values that b0 and b1 take during the iteration
beta.reposit <- data.frame(b0=beta.mat[1],b1=beta.mat[2])
while(!beneath_level) {
  N.mat <- 1/(1+exp(-t(beta.mat) %*% t(X.mat)))
  grad.ll <- t(X.mat) %*% (y.mat - t(N.mat))
  Hess.ll <- -t(X.mat) %*% diag(t(N.mat*(1 - N.mat))[,1]) %*% X.mat
  # Calculate b0_hat en b1_hat
  hhat.mat <- solve(Hess.ll,-grad.ll)
  bhat.mat <- beta.mat + hhat.mat
  # Add new value to the repository of p
  beta.reposit <- rbind(beta.reposit,c(bhat.mat[1],bhat.mat[2]))
  # Calculate the first derivative and check if it is close enough to zero (beneath the level)
  N.mat <- 1/(1+exp(-t(bhat.mat) %*% t(X.mat)))
  grad.ll <- t(X.mat) %*% (y.mat - t(N.mat))
  if (sqrt(grad.ll[1]^2 + grad.ll[2]^2) < level) {
    beneath_level <- TRUE
  }
  # Put the new value of beta into beta.mat
  beta.mat <- bhat.mat
}
paste("The numerical solutions are: b0=",beta.mat[1]," b1=",beta.mat[2])
paste("The results of the logistic regression with R are:")
paste("b0=",summary(glm(y~x,binomial))$coefficients[1,1]," and b1=",summary(glm(y~x,binomial))$coefficients[2,1])
beta.reposit
@

\chapter{Function in k variables}
Essentially the method is the same. As an example we use a logistic regression, linear in $k$ variables. The \emph{Log Likelihood function} is then given by:
\begin{equation}
\ell((y_{1},y_{2}, \ldots y_{n})|\beta_{0}=\beta_{0}^{*},\beta_{1}=\beta_{1}^{*} \ldots \beta_{k}=\beta_{k}^{*})=n.ln(\frac{1}{\sqrt{2\pi}\sigma}) + \sum_{i=1}^{i=n} -\frac{1}{2}\left( \frac{y_{1}-\beta_{0}^{*}-\beta_{1}^{*}x_{1,i}-\beta_{2}^{*}x_{2,i} \ldots -\beta_{k}^{*}x_{k,i}}{\sigma} \right) ^{2}
\end{equation}

You pick $k$ starting variables $\boldsymbol{\beta}=[\beta_{0}^{*},\beta_{1}^{*}, \ldots, \beta_{k-1}^{*}]$ and then solve
\begin{equation}
\textbf{Hess}(\ell(\beta_{0}^{*},\beta_{1}^{*}, \ldots, \beta_{k-1}^{*})) \boldsymbol{\cdot} \textbf{h} = - \boldsymbol{\nabla}(\ell(\beta_{0}^{*},\beta_{1}^{*}, \ldots, \beta_{k-1}^{*}))
\end{equation}
for $\textbf{h}$

where
\begin{equation}
\begin{split}
\boldsymbol{\nabla}(\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*})) &= \begin{bmatrix} \frac{\partial}{\partial \beta_{0}}\ell(\beta_{0}^{*},\beta_{1}^{*}, \ldots \beta_{k}^{*}) \\ \frac{\partial}{\partial \beta_{1}}\ell(\beta_{0}^{*},\beta_{1}^{*}, \ldots \beta_{k}^{*}) \\ \vdots \ \frac{\partial}{\partial \beta_{k}}\ell(\beta_{0}^{*},\beta_{1}^{*}, \ldots, \beta_{k}^{*}) \end{bmatrix} \\
\textbf{Hess}(\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*})) &= \begin{bmatrix} \frac{\partial^{2}}{\partial \beta_{0}^{2}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) & \frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{1}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) & \ldots & \frac{\partial^{2}}{\partial \beta_{0} \partial \beta_{k}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) \\ \frac{\partial^{2}}{\partial \beta_{1} \partial \beta_{0}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) & \frac{\partial^{2}}{\partial \beta_{1}^{2}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) & \ldots & \frac{\partial^{2}}{\partial \beta_{1} \partial \beta_{k}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) \\ \vdots & \vdots & \hdots & \vdots \\ \frac{\partial^{2}}{\partial \beta_{k} \partial \beta_{0}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) & \frac{\partial^{2}}{\partial \beta_{k} \partial \beta_{1}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) & \ldots & \frac{\partial^{2}}{\partial \beta_{k}^{2}}\ell(\beta_{0}^{*}, \beta_{1}^{*}, \ldots \beta_{k}^{*}) \end{bmatrix}
\end{split}
\end{equation}
\medskip

The next iteration is done with the new values of $\boldsymbol{\beta}_{new}=\boldsymbol{\beta}_{old} + \textbf{h}$. All this work is done in R with the \textit{glm-function}.

\newpage
\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}